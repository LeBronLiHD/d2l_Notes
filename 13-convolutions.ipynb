{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913a6f1",
   "metadata": {},
   "source": [
    "# From Fully-Connected Layers to Convolutions\n",
    "\n",
    "- To start off, we can consider an MLP with two-dimensional images $X$ as inputs and their immediate hidden representations $HH$ similarly represented as matrices in mathematics and as two-dimensional tensors in code, where both $X$ and $H$ have the same shape. Let that sink in. We now conceive of not only the inputs but also the hidden representations as possessing spatial structure\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{[\\mathbf{H}]_{i, j} } &=[\\mathbf{U}]_{i, j}+\\sum_{k} \\sum_{l}[\\mathbf{W}]_{i, j, k, l}[\\mathbf{X}]_{k, l} \\\\\n",
    "&=[\\mathbf{U}]_{i, j}+\\sum_{a} \\sum_{b}[\\mathbf{V}]_{i, j, a, b}[\\mathbf{X}]_{i+a, j+b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Now let us invoke the first principle established above: **translation invariance**. This implies that a shift in the input $X$ should simply lead to a shift in the hidden representation $H$\n",
    "- This is only possible if $V$ and $U$ do not actually depend on $(i, j)$, i.e.\n",
    "\n",
    "$$\n",
    "[\\mathbf{H}]_{i, j}=u+\\sum_{a} \\sum_{b}[\\mathbf{V}]_{a, b}[\\mathbf{X}]_{i+a, j+b}\n",
    "$$\n",
    "\n",
    "- Now let us invoke the second principle: **locality**. As motivated above, we believe that we should not have to look very far away from location $(i, j)$ in order to glean relevant information to assess what is going on at $[\\mathbf{H}]_{i, j}$.\n",
    "\n",
    "$$\n",
    "[\\mathbf{H}]_{i, j}=u+\\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta}[\\mathbf{V}]_{a, b}[\\mathbf{X}]_{i+a, j+b}\n",
    "$$\n",
    "\n",
    "- fully connected layer + translation + locality = convolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588c042",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "\n",
    "![convolution](./images/convolution.png)\n",
    "\n",
    "- two dimension Cross-Correlation\n",
    "\n",
    "$$\n",
    "y_{i, j}=\\sum_{a=1}^{h} \\sum_{b=1}^{w} w_{a, b} x_{i+a, j+b}\n",
    "$$\n",
    "\n",
    "- two dimension convolution\n",
    "\n",
    "$$\n",
    "y_{i, j}=\\sum_{a=1}^{h} \\sum_{b=1}^{w} w_{-a, -b} x_{i+a, j+b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c311d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3dea3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4ac2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9a9d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f43609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])\n",
    "Y = corr2d(X, K)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060b4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c2d40ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 4.642\n",
      "epoch 2, loss 2.316\n",
      "epoch 3, loss 1.214\n",
      "epoch 4, loss 0.667\n",
      "epoch 5, loss 0.382\n",
      "epoch 6, loss 0.226\n",
      "epoch 7, loss 0.137\n",
      "epoch 8, loss 0.085\n",
      "epoch 9, loss 0.053\n",
      "epoch 10, loss 0.033\n"
     ]
    }
   ],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example, channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # Learning rate\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # Update the kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df28cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0130, -0.9758]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
